{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/Volumes/GoogleDrive/My Drive/Colab Notebooks/NeuralNetwork\")\n",
    "from library_model import layers as lay\n",
    "from library_model import model_building as mb\n",
    "from library_model import model_training as mt\n",
    "from data import data_loading as dt\n",
    "from data import text as txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = sum((1 for _ in train_iter))\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [lay.Network_state() for _ in range(5)]\n",
    "for k, state in enumerate(states):\n",
    "    p= state.parameters\n",
    "    tr= state.training\n",
    "\n",
    "    p.d_model=200+50*k\n",
    "    p.d_hid=200+50*k\n",
    "    p.nheads=2 + k//2\n",
    "    p.d_key = p.d_model //p.nheads\n",
    "    p.nlayers=2\n",
    "    p.network = \"encoder\"\n",
    "\n",
    "\n",
    "    tr.batch_size=20\n",
    "    tr.data_split = 0.8 #fraction of dataset that will be used for training vs validation\n",
    "    tr.lr = 1.\n",
    "    tr.seq_length=35\n",
    "    tr.optimizer = \"sgd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [dt.Data() for _ in range(len(states))]\n",
    "for d, s in zip(datasets, states):\n",
    "    params = lay.param_count(s)\n",
    "    s.training.data_fraction = lay.param_count(states[0])/params\n",
    "    d.tokenizer, d.train_dataloader, d.test_dataloader = dt.process_data(train_iter, txt.library_text_coders, s, network= \"encoder\")\n",
    "    s.parameters.ntokens = s.parameters.ntokens_out = len(d.tokenizer.vocab)\n",
    "    s.training.schedule = mb.learning_rate_step(factor=5., drop=2., time=1.)\n",
    "\n",
    "dataset_sizes = [len(d.train_dataloader) for d in datasets]\n",
    "parameter_counts = list( map(lay.param_count, states))\n",
    "compute = [6*a*b*0.8 for a, b in zip( dataset_sizes, parameter_counts)]\n",
    "print(f\"{compute[0]:.2E}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "models = []\n",
    "trainers = []\n",
    "for state in states:\n",
    "    parts = mb.get_transformer_parts(state)\n",
    "    model = lay.EncoderModel(parts.encoder, parts.linear).to(state.device)\n",
    "    opt, scheduler = mb.get_optimizer(state, model)\n",
    "    trainer = mt.Model_training(model, opt, criterion, scheduler)\n",
    "    models.append(model)\n",
    "    trainers.append(trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainers[0].scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainers[0](epochs = 5, data = datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f1af38ef70d2ba974426d7a8a6b596b940002c76a66dd7cd7f620996d04d624"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
