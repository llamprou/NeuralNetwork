{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9krcomO4EWLv"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import math\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "iaAUXihrEivV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "import time"
      ],
      "metadata": {
        "id": "pibl-bcgEk1W"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My custom dataloaders"
      ],
      "metadata": {
        "id": "IS_sLfCoFChh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "#Building an iterator over minibatches of inputed text after tensor encoding\n",
        "class Dataloader_iter(object):\n",
        "  def __init__(self, input, output, batch_size, shuffle_batch=False, inp_transformation=None, out_transformation=None, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "    if inp_transformation is not None:\n",
        "      self.input = inp_transformation(input).to(device)\n",
        "    else:\n",
        "      self.input = input.to(device)\n",
        "\n",
        "    if out_transformation is not None:\n",
        "      self.output = out_transformation(output).to(device)\n",
        "    else:\n",
        "      self.output = output.to(device)\n",
        "    \n",
        "    self.shuffle = shuffle_batch\n",
        "    self.batch_size = batch_size\n",
        "    self.length= len(self.input)\n",
        "\n",
        "  def __iter__(self):\n",
        "    self.iter_idx = 0\n",
        "    self.idx = [k for k in range(self.length//self.batch_size)]\n",
        "    if self.shuffle:\n",
        "      random.shuffle(self.idx)\n",
        "\n",
        "    while (self.iter_idx < int(self.length/self.batch_size)):\n",
        "      sample =  self.idx[self.iter_idx-1]\n",
        "      yield self.input[sample * self.batch_size : (sample+1) * self.batch_size], self.output[sample * self.batch_size : (sample+1) * self.batch_size]\n",
        "      self.iter_idx += 1\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(self.length/self.batch_size)\n",
        "      \n",
        " \n",
        "\n",
        "#transformer dataloader\n",
        "def get_tranformer_dataloader(*args, **kwargs):\n",
        "  generator = Dataloader_iter(*args, **kwargs)\n",
        "  class new_generator():\n",
        "    def __init__(self, generator):\n",
        "      self.generator = generator\n",
        "      self.len = len(generator)\n",
        "    \n",
        "    def __iter__(self):\n",
        "      self.iter_idx = 0\n",
        "      iterator = iter(self.generator)\n",
        "      while self.iter_idx < self.len:\n",
        "        x,y = next(iterator)\n",
        "        yield ([x, y[:, :-1]], y[:, 1:])\n",
        "        self.iter_idx +=1\n",
        "    def __len__(self):\n",
        "      return self.len\n",
        "\n",
        "  return new_generator(generator)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "X_0qDxX6Es1f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My custom tokenizer and text en/decoder"
      ],
      "metadata": {
        "id": "VbPKpl0gFGja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "#Word level tokenizer and text encoder/decoder\n",
        "class My_word_tokenizer():\n",
        "  def __init__(self, text):\n",
        "    self.punctuations = list(\".,?:[]()!;/,-_}{\")\n",
        "    if (type(text)!=str):\n",
        "      text = \" \".join([elem for elem in text])\n",
        "    transformed_text = self.punctuation(text, \"split\")\n",
        "    self.tokens = list(set(transformed_text.split(\" \"))) \n",
        "    self.token_encoding = {token : torch.tensor([k]).unsqueeze(0) for k, token in enumerate(self.tokens)}\n",
        "    self.token_onehot ={k : [1 if l ==k else 0 for l in range(len(self.tokens))] for k in range(len(self.tokens))}\n",
        "      \n",
        "    \n",
        "  #method for placing gaps before punctuation signs so that they will tokenized like words during encoding\n",
        "  #and also for removing same gaps before text decoding.\n",
        "  def punctuation(self, text, mode):\n",
        "    for p in self.punctuations:\n",
        "      if mode == \"split\":\n",
        "        text=text.replace(p, \" \"+p) \n",
        "      elif mode == \"join\":\n",
        "        text = text.replace(\" \"+p , p)\n",
        "      else:\n",
        "        print(\"Mode selection error\")\n",
        "    return text \n",
        "\n",
        "\n",
        "  def text_encoding(self, text):\n",
        "    text = (self.punctuation(text, \"split\")).split(\" \")\n",
        "    return torch.cat([self.token_encoding[word] for word in text], dim=1)\n",
        "\n",
        "\n",
        "  #greedy decoding of the text\n",
        "  def text_decoding(self, output):\n",
        "    text = \" \".join(map(str, [list(self.token_encoding.keys())[word.detach()] for word in output.squeeze()]))\n",
        "    text = self.punctuation(text, \"join\")\n",
        "    return text\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "D2HA7VQRE3Sg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NN Layers"
      ],
      "metadata": {
        "id": "rqVN-7EwFNqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "#define linear layer class\n",
        "class linear_layer(nn.Module):\n",
        "    def __init__(self, hyper_param): #(inp_dim, hid_dim, bias_is_true, relu_is_true)\n",
        "        super().__init__()\n",
        "        (self.inp_dim, self.hid_dim, self.bias_is_true, self.relu_is_true) = hyper_param\n",
        "        self.weight = nn.Parameter(torch.randn(self.inp_dim, self.hid_dim)/torch.sqrt(torch.tensor(self.inp_dim)))\n",
        "        if self.bias_is_true:\n",
        "            self.bias = nn.Parameter(torch.randn(self.hid_dim))\n",
        "        self.relu =nn.ReLU()\n",
        "        \n",
        "    def forward(self, input):\n",
        "        output= torch.tensordot(input, self.weight,  dims = ([-1],[0]) ) \n",
        "        if self.bias_is_true:\n",
        "            output+= self.bias\n",
        "        if self.relu_is_true:\n",
        "            output = self.relu(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------------\n",
        "#define FeedForward class\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, hyper_param): #([(dim_in, dim_out, bias_is_true, relu_is_true) ,(), ... ])\n",
        "        nn.Module.__init__(self) \n",
        "        self.hyper_param = hyper_param\n",
        "        self.layers = nn.ModuleList([linear_layer(param) for param in self.hyper_param])\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = input\n",
        "        for layer in self.layers:\n",
        "            output = layer(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class attention(nn.Module):\n",
        "    def __init__(self, hyper_param): #(dim_in, dim_key, dim_heads)\n",
        "        nn.Module.__init__(self) \n",
        "        (self.dim_in, self.dim_key, self.heads) = hyper_param\n",
        "        self.attention = nn.ModuleList([linear_layer((self.dim_in, self.dim_key*self.heads, False, False)) for _ in range(3)])\n",
        "        self.final = linear_layer((self.heads * self.dim_key, self.dim_in, False, False))\n",
        "        self.softmax = nn.Softmax(dim = -1)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "    \n",
        "    def forward(self, inputs, mask=None):\n",
        "        q,k,v = tuple(layer(inp).view(inp.size(0), inp.size(1), self.heads, self.dim_key).transpose(1,2) for inp, layer in zip(inputs, self.attention))\n",
        "        score = torch.matmul(q,k.transpose(-1,-2)).masked_fill(mask==0, -1e9) if mask is not None else torch.matmul(q,k.transpose(-1,-2))\n",
        "        p_atten = self.dropout(self.softmax(score/torch.sqrt(torch.tensor(self.dim_key))))\n",
        "        preactivation = torch.matmul(p_atten,v).transpose(1,2).reshape(v.size(0), -1, self.heads*self.dim_key)\n",
        "        return self.final(preactivation)\n",
        "\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------------------------\n",
        "#define layer norm class\n",
        "#The Annotated transformer adds 2 extra learnable parameters in this layer --I don't.\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.epsilon= 10**(-7)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        mean = torch.mean(input, dim= -1, keepdim= True)\n",
        "        std = torch.std(input, dim=-1, keepdim= True) + self.epsilon\n",
        "        output = (input - mean)*(1/std)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------------\n",
        "#Skip connection and layer normalization decorator + dropout \n",
        "def SkipAndNormalize_decorator(cls):\n",
        "    class ResNorm_wrapper(nn.Module):\n",
        "        def __init__(self, hyper_param):\n",
        "            super().__init__()\n",
        "            self.layers = nn.ModuleList([cls(hyper_param), LayerNorm()])\n",
        "            self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        \n",
        "        def forward(self, residual_stream, *input):\n",
        "            h1 = self.layers[0](*input) + residual_stream\n",
        "            return self.dropout(self.layers[1](h1))\n",
        "    return ResNorm_wrapper    \n",
        "\n",
        "\n",
        "\n",
        "@SkipAndNormalize_decorator\n",
        "class SkipAttention(attention):\n",
        "    def __init__(self, hyper_param): #(dim_in, dim_key, dim_heads)\n",
        "        super().__init__(hyper_param)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "@SkipAndNormalize_decorator\n",
        "class SkipFeedForward(FeedForward):\n",
        "    def __init__(self, hyper_param): #[(dim_in, dim_out, bias_is_true, relu_is_true),(),()]\n",
        "        super().__init__(hyper_param)\n",
        "        \n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------------\n",
        "#Helper function for getting attention and FF blocks for building En/De-coders, Transformers\n",
        "def get_coder_blocks(dim_in, dim_key, heads, dim_internal, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "    att_hyperparams = (dim_in, dim_key, heads)\n",
        "    ff_hyperparams = [(dim_in, dim_internal, True, True), (dim_internal, dim_in, True, False)]\n",
        "    return SkipAttention(att_hyperparams).to(device), SkipFeedForward(ff_hyperparams).to(device)\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------------\n",
        "#define encoder class\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, skip_attention, skip_feedforward):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([skip_attention, skip_feedforward])\n",
        "\n",
        "    \n",
        "    def forward(self, input, mask=None): \n",
        "        inputs = [input for _ in range(3)]\n",
        "        h1= self.layers[0](input, inputs, mask)\n",
        "        output = self.layers[1](h1, h1)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "# define decoder class\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, skip_attention, skip_feedforward):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([skip_attention, copy.deepcopy(skip_attention), skip_feedforward ])\n",
        "    \n",
        "\n",
        "    def forward(self, input, enc_mask=None, dec_mask=None): #input = list [encoder_output, decoder_input]\n",
        "        encoder_output = input[0]\n",
        "        decoder_input = input[1]\n",
        "        h1 = self.layers[0](decoder_input, [decoder_input for _ in range(3)], dec_mask)\n",
        "        h2 = self.layers[1](h1, [h1, encoder_output, encoder_output], enc_mask)\n",
        "        output = self.layers[2](h2, h2)\n",
        "        return [encoder_output, output]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------------\n",
        "#define encoder/decoder stack decorator\n",
        "def get_stack(cls):\n",
        "    class Stack_wrapper(nn.Module):\n",
        "        def __init__(self, skip_attention, skip_feedforward, copies):\n",
        "            super().__init__()\n",
        "            blocks = [[copy.deepcopy(skip_attention), copy.deepcopy(skip_feedforward)] for _ in range(copies)]\n",
        "            self.layers = nn.ModuleList([cls(*block) for block in blocks])\n",
        "\n",
        "        def forward(self, input, *masks): #there can be 1 or 2 masks depending on whether we are stacking encoders or decoders\n",
        "            output = input\n",
        "            for layer in self.layers:\n",
        "                output = layer(output, *masks)\n",
        "            return output\n",
        "    return Stack_wrapper\n",
        "            \n",
        "\n",
        "@get_stack\n",
        "class EncoderStack(Encoder):\n",
        "    def __init__(self, skip_attention, skip_feedforward):\n",
        "        super().__init__(skip_attention, skip_feedforward)\n",
        "\n",
        "\n",
        "@get_stack\n",
        "class DecoderStack(Decoder):\n",
        "    def __init__(self, skip_attention, skip_feedforward):\n",
        "        super().__init__(skip_attention, skip_feedforward)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------------\n",
        "#transformer layer class\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, skip_attention, skip_feedforward, copies, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "        nn.Module.__init__(self) \n",
        "        self.layers = nn.ModuleList([EncoderStack(skip_attention, skip_feedforward, copies).to(device), DecoderStack(copy.deepcopy(skip_attention), copy.deepcopy(skip_feedforward), copies).to(device)])\n",
        "\n",
        "\n",
        "    def forward(self, inputs, enc_mask=None, dec_mask=None): #inputs is a list [encoder input, decoder input]\n",
        "        h1 = self.encode(inputs[0], enc_mask)\n",
        "        return self.decode([h1, inputs[1]], enc_mask, dec_mask)\n",
        "\n",
        "\n",
        "    def encode(self, input, enc_mask=None):\n",
        "        return self.layers[0](input, enc_mask)\n",
        "\n",
        "    def decode(self, inputs, enc_mask=None, dec_mask=None):\n",
        "        output = self.layers[1](inputs, enc_mask, dec_mask)\n",
        "        return output[1]\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------------\n",
        "#Positional encoding class\n",
        "class Positional_enc(nn.Module):\n",
        "    def __init__(self, dim_in, max_dim=500):\n",
        "        nn.Module.__init__(self)\n",
        "        self.dim_in, self.max_dim = dim_in, max_dim\n",
        "        #construct positional encoding for single batch element\n",
        "        argument = torch.tensordot(torch.arange(max_dim, dtype=torch.float), torch.exp(-math.log(1000) *torch.arange(0, dim_in, 2, dtype=torch.float)/dim_in) , dims= 0)\n",
        "        pos_enc= torch.empty(max_dim, dim_in)\n",
        "        pos_enc[:, 0::2] = torch.sin(argument)\n",
        "        pos_enc[:, 1::2] = torch.cos(argument)\n",
        "        #introduce batch dimension (=0) \n",
        "        pos_enc = pos_enc.unsqueeze(0)\n",
        "        self.register_buffer(\"pos_enc\", pos_enc)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        input =input + self.pos_enc[:, :input.size(1), :].requires_grad_(False)\n",
        "        return self.dropout(input)\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------------\n",
        "#Mask\n",
        "def construct_mask(size, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "    uppertri = torch.triu(torch.ones(1, size, size), diagonal=1)\n",
        "    return (uppertri ==0).to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------------\n",
        "#include positional encoding, masking, and softmax\n",
        "class Transformer_model(nn.Module):\n",
        "    def __init__(self, position_enc, enc_embedding, dec_embedding, transformer, linear, start, end):\n",
        "        nn.Module.__init__(self) \n",
        "        self.layers = nn.ModuleList([enc_embedding, dec_embedding, transformer, linear])\n",
        "        self.position_enc = [position_enc, copy.deepcopy(position_enc)]\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "\n",
        "    def embed_encode(self, input, enc_mask=None):\n",
        "        h1 = self.layers[0](input)\n",
        "        encoder_input = self.position_enc[0](math.sqrt(h1.size(-1))*h1)\n",
        "        enc_output = self.layers[2].encode(encoder_input, enc_mask)\n",
        "        return enc_output \n",
        "    \n",
        "    def embed_decode(self, enc_output, dec_input, enc_mask=None): #inputs is a list [enc_input, dec_input]\n",
        "        h2 = self.layers[1](dec_input)\n",
        "        h2= self.layers[2].decode([enc_output,  self.position_enc[1](math.sqrt(h2.size(-1))*h2 )], enc_mask, construct_mask(h2.size(1)) ) \n",
        "        return self.layers[3](h2)\n",
        "\n",
        "    def forward(self, inputs, enc_mask=None): #inputs = [enc_input, dec_input] returns a tuple of outputs and probabilities\n",
        "        enc_output = self.embed_encode(inputs[0])\n",
        "        presoftmax_out = self.embed_decode(enc_output, inputs[1], enc_mask)\n",
        "        probabilities = self.softmax(presoftmax_out) \n",
        "        output_seq = torch.argmax(probabilities, dim =-1)\n",
        "        return output_seq, presoftmax_out\n",
        "\n",
        "\n",
        "    def autoregression(self, input, length, enc_mask = None, mode= \"greedy\"): #input= encoder_input (1 x seq_length)\n",
        "        enc_output = self.embed_encode(input)\n",
        "        inp = self.start #(batch_size x seq_length) \n",
        "        for _ in range(length):\n",
        "            prob = self.softmax(self.embed_decode(enc_output, inp, enc_mask)) \n",
        "            \n",
        "            # Greedy inference\n",
        "            if (mode == \"greedy\"):\n",
        "                out = torch.argmax(prob, dim =-1)\n",
        "                next_word = (torch.tensor([out[:, -1]]).unsqueeze(0) ).to(out, non_blocking=True)\n",
        "                inp = torch.cat([inp, next_word], dim=1)\n",
        "                if next_word == self.end:\n",
        "                    break\n",
        "                else: \n",
        "                    continue\n",
        "            \n",
        "            # Beam search using Bayesian inference for next two words A_1, A_2 \n",
        "            # Our strategy is: Starting from the two most likely tokens for A_1, we predict the most likely next token A_2max(A_1) given each choice and then select the A_1 that maximizes P(A_1)*P(A_2max | A_1)\n",
        "            # In this way, the next word A_1 is chosen so that it maximizes P(A_1 U A_2)\n",
        "            elif (mode == \"beam\"):\n",
        "                largest_two_probabilities, likely_words = torch.topk(prob[:,-1], 2, dim= -1)\n",
        "                predicted_batch = torch.cat([inp, inp], dim =0)\n",
        "                predicted_batch=torch.cat([predicted_batch, likely_words.transpose(0,1)], dim=1)\n",
        "                new_enc_output = torch.cat([enc_output]*2, dim=0)\n",
        "                h = self.softmax(self.embed_decode(new_enc_output, predicted_batch.type(torch.long), enc_mask))[:,-1] \n",
        "                next_probabilities, _ = torch.max(h.detach(), dim = -1)\n",
        "\n",
        "                most_likely_word_idx =(torch.argmax(largest_two_probabilities.squeeze()* next_probabilities)).item()\n",
        "                next_word = likely_words[:, most_likely_word_idx].unsqueeze(0)\n",
        "                inp = torch.cat([inp, next_word], dim=1)\n",
        "                if next_word == self.end:\n",
        "                    break\n",
        "                else: \n",
        "                    continue\n",
        "        return inp\n",
        "        \n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "P7ZNxWGoE_J3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tranformer model construction helper functions"
      ],
      "metadata": {
        "id": "4nOCBguGFQYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "#Transformer model \n",
        "def get_registered_Transformer_model(in_vocab_size, out_vocab_size, dim_in, dim_key, heads, dim_internal, copies, lr, start, end, optimizer = \"sgd\", device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "    attention_block, FF_block = get_coder_blocks(dim_in, dim_key, heads, dim_internal)\n",
        "    transformer = TransformerLayer(attention_block, FF_block, copies).to(device)\n",
        "    position_enc = Positional_enc(dim_in, max_dim= 5000).to(device)\n",
        "    linear = linear_layer((dim_in, out_vocab_size, False, False)).to(device)\n",
        "    enc_embedding=nn.Embedding(in_vocab_size, dim_in).to(device)\n",
        "    dec_embedding=nn.Embedding(out_vocab_size, dim_in).to(device)\n",
        "\n",
        "    model = Transformer_model(position_enc, enc_embedding, dec_embedding, transformer, linear, start, end).to(device)\n",
        "    #initializing according to the transformer paper\n",
        "    for p in model.parameters(): \n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    if optimizer == \"adam\":\n",
        "        opt = optim.Adam(model.parameters(), lr, betas=(0.9, 0.98), eps=1e-9)\n",
        "    elif optimizer == \"sgd\":\n",
        "        opt = optim.SGD(model.parameters(), lr)\n",
        "    else:\n",
        "        print(\"Optimizer choice not recognized\")\n",
        "    \n",
        "    return model, opt \n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "h1Jj-WIyFkK5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model training class"
      ],
      "metadata": {
        "id": "bqxtLZlBF8kB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "class Model_training(nn.Module):\n",
        "    def __init__(self, model, opt, loss):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.opt =opt\n",
        "        self.loss = loss\n",
        "    \n",
        "\n",
        "    def run_transformer_epoch(self, train_dataloader, tokenizer, device):\n",
        "        self.model.train()\n",
        "        train_iterator = iter(train_dataloader)\n",
        "        total_loss =0\n",
        "        t1= time.time()\n",
        "        for input_batch, output_batch in train_iterator:\n",
        "            _, out_prob = self.model(input_batch)\n",
        "\n",
        "            #Convert output words from integers to one-hot vectors ---I could do this separately and define a dictionary to increase speed\n",
        "            expectation = torch.tensor([[tokenizer.token_onehot[int(seq_elem)] for seq_elem in batch_elem] for batch_elem in output_batch], dtype=torch.float).to(device)\n",
        "\n",
        "            loss = self.loss(out_prob, expectation).to(device)\n",
        "            total_loss += loss\n",
        "            \n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "            self.opt.zero_grad()\n",
        "        if self.scheduler is not None:\n",
        "            self.scheduler.step()\n",
        "        t2= time.time() \n",
        "        return t2-t1, total_loss   \n",
        "\n",
        "\n",
        "\n",
        "    def evaluate_transformer_output(self, tokenizer, sample_input):\n",
        "        self.model.eval()\n",
        "        text_instance_encoded = self.model.autoregression(sample_input, 62)\n",
        "        if tokenizer is not None:\n",
        "            output = tokenizer.text_decoding(text_instance_encoded)\n",
        "        else:\n",
        "            output = text_instance_encoded\n",
        "        return output\n",
        "    \n",
        "\n",
        "    def fit_transformer(self, epochs, train_dataloader, test_dataloader=None, tokenizer = None, sample_input=None, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "        for epoch in range(epochs):\n",
        "            dt, epoch_loss = self.run_transformer_epoch(train_dataloader, tokenizer, device)\n",
        "            print(f\"Epoch: {epoch} -- time: {dt} -- loss = {epoch_loss}\\n\")\n",
        "\n",
        "            output = self.evaluate_transformer_output(tokenizer, sample_input)\n",
        "            print(output)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "j8xcKVHyF_W2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NN functionalities class"
      ],
      "metadata": {
        "id": "ULWSFw9CGOpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# class for model operations \n",
        "# USE .pt FOR SAVING MODELS!! The other methods seem to generate small errors in the saved tensors that ruin saved network performance.\n",
        "class NN_operating_tools(Model_training, nn.Module):\n",
        "    def __init__(self, model, opt, learning_rate_schedule=None, saved_model=None): #learning_rate_schedule must be a lambda function\n",
        "        nn.Module.__init__(self)\n",
        "        self.model = model\n",
        "        self.opt =opt\n",
        "        if learning_rate_schedule is not None:\n",
        "            self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.opt, learning_rate_schedule)\n",
        "        else:\n",
        "            self.scheduler =None\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "        Model_training.__init__(self, model, opt, self.loss)\n",
        "\n",
        "        if saved_model is not None:\n",
        "            self.load_model(saved_model)\n",
        "\n",
        "    \n",
        "    def load_model(self, saved_model):\n",
        "        dir =os.path.join(\"saved_models\", saved_model)\n",
        "        file_opener={\".csv\": self.load_model_csv, \".pt\": self.load_model_pt, \".npy\" : self.load_model_npy, \".pkl\" : self.load_model_pkl}\n",
        "        idx = saved_model.find(\".\")\n",
        "        file_opener[saved_model[idx:]](dir)\n",
        "\n",
        "\n",
        "    def load_model_pt(self, directory):\n",
        "        t_load = torch.load(directory)\n",
        "        self.model.load_state_dict(t_load)\n",
        "\n",
        "    def load_model_csv(self, dir):\n",
        "        load_model = pd.read_csv(dir)\n",
        "        for idx, param in enumerate(self.model.parameters()):\n",
        "            with torch.no_grad():\n",
        "                param.copy_(torch.tensor(load_model.iloc[idx, 1]))\n",
        "\n",
        "\n",
        "    def load_model_npy(self, dir):\n",
        "        load_model = np.load(dir, allow_pickle=True)\n",
        "        for k, param in enumerate(self.model.parameters()):\n",
        "            with torch.no_grad():\n",
        "                param.copy_(torch.from_numpy(load_model[k]))\n",
        "\n",
        "\n",
        "    def load_model_pkl(self, dir):\n",
        "        with open(dir, \"rb\") as f:\n",
        "            load_model = pickle.load(f)\n",
        "        for k, param in enumerate(self.model.parameters()):\n",
        "                with torch.no_grad():\n",
        "                    param.copy_(torch.tensor(load_model[k]))\n",
        "\n",
        "    \n",
        "    def save_model(self, name):\n",
        "        dir =os.path.join(\"saved_models\", name)\n",
        "        file_saver={\".csv\": self.save_csv, \".pt\": self.save_pt, \".npy\" : self.save_npy, \".pkl\" : self.save_pkl}\n",
        "        idx = name.find(\".\")\n",
        "        file_saver[name[idx:]](dir)\n",
        "        \n",
        "\n",
        "    def save_pt(self, directory):\n",
        "        torch.save(copy.deepcopy(self.model.state_dict()), directory)\n",
        "\n",
        "\n",
        "    def save_npy(self, directory):\n",
        "        trained_model = [copy.deepcopy(p.detach()).numpy() for p in self.model.parameters()]\n",
        "        np.save(directory, trained_model)\n",
        "\n",
        "    def save_csv(self,directory):\n",
        "        pass\n",
        "\n",
        "    def save_pkl(self,directory):\n",
        "        pass\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2MOq57riGV6u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data"
      ],
      "metadata": {
        "id": "3aLP_cJ7GY0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mytextpiece = \"<start> When I was young, I never needed anyone and making love was just for fun. Those days are gone! Living alone, I think of all the friends I've known but when I dial the telephone nobody is home. All by myself, I don't wanna be all by myself, anymore! <end>\"\n",
        "mytranslationpiece = \"<start> Οταν ημουν νεος, δεν χρειαζομουν κανενα και το να κανω ερωτα ηταν απλα για ευχαριστηση. Αυτες οι μερες εχουν περασει! Ζωντας μονος μου, σκεφτομαι ολους τους φιλους που ξερω αλλα οταν τους καλω στο τηλεφωνο κανεις δεν ειναι σπιτι. Ολομοναχος, δε θελω πια να ειμαι ολομοναχος!\"\n",
        "mytext = [mytextpiece for _ in range(2000)]\n",
        "mytranslation = [mytranslationpiece for _ in range(2000)]"
      ],
      "metadata": {
        "id": "yknfLQsdGpuu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data processing helper functions"
      ],
      "metadata": {
        "id": "cBIgSpPZGqLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# input/output texts are expected to be lists of text pieces, like sentences/paragraphs/books to be translated, etc. \n",
        "# All list elements are assumed to be word sequences of the same length. If not, a fake token needs to be introduced to ensure that.\n",
        "def get_tranformer_data(input_text, output_text, batch_size, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "  tokenizer_in = My_word_tokenizer(\"\".join(input_text))\n",
        "  tokenizer_out = My_word_tokenizer(\"\".join(output_text))\n",
        "  encoded_input = torch.cat([tokenizer_in.text_encoding(in_piece) for in_piece in input_text], dim = 0)\n",
        "  encoded_output = torch.cat([tokenizer_out.text_encoding(out_piece) for out_piece in output_text], dim = 0)\n",
        "  dataloader = get_tranformer_dataloader(encoded_input, encoded_output, batch_size, True)\n",
        "  start = tokenizer_out.token_encoding[\"<start>\"].to(device)\n",
        "  end = tokenizer_out.token_encoding[\"<end>\"].to(device)\n",
        "  return tokenizer_in, tokenizer_out, dataloader, start, end\n",
        "\n",
        "def get_dataloader(data, seq_length, batch_size):\n",
        "  data = data[:(len(data)//seq_length)*seq_length].view(-1,seq_length) #organizes flat data tensor to fixed-length sequences\n",
        "  return get_tranformer_dataloader(data, data, batch_size, shuffle_batch=True) #creates dataloader with random shuffling and fixed batch size\n",
        "\n",
        "def process_data(train_iter, coders_cls, seq_length, batch_size):\n",
        "  coders = coders_cls(train_iter)  #generates vocab, contains tokenizer, text encoders and decoders\n",
        "  text_data = \" \".join(list(item for item in train_iter))  #merges text items of Wikitext2 generator to form single text\n",
        "  train_data = coders.text_encoding(text_data).view(-1).to(device)  #encodes text into flat tensor and sends it to device\n",
        "  return coders, get_dataloader(train_data, seq_length, batch_size)  "
      ],
      "metadata": {
        "cellView": "form",
        "id": "0c3rLpuAGvJ7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get dataloaders"
      ],
      "metadata": {
        "id": "q-N7PZKMIY-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_in, tokenizer_out, dataloader, starting_wrd, ending_wrd = get_tranformer_data(mytext, mytext, 80)\n",
        "encode=tokenizer_in.text_encoding\n",
        "decode=tokenizer_out.text_decoding"
      ],
      "metadata": {
        "id": "UZaLhRAaIf8P"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters"
      ],
      "metadata": {
        "id": "ic7x2hwiHu9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = tokenizer_in.tokens\n",
        "ntokens = len(vocab)  # size of vocabulary\n",
        "emsize = 60  # embedding dimension\n",
        "d_hid = 80  # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 3  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 6"
      ],
      "metadata": {
        "id": "Tf_p1pSGHwPJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Schedule"
      ],
      "metadata": {
        "id": "ZdDjPXBOH4sB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# example of a learning rate schedule used in Annotated Transformer\n",
        "def learning_rate_function(model_size, factor, warmup_steps):\n",
        "    return lambda epoch : factor* (model_size)**(-0.5) * min((epoch+1)**(-0.5), (epoch+1) * (warmup_steps)**(-1.5)) \n",
        "\n",
        "def learning_rate_step(factor, drop, time):\n",
        "    return lambda epoch : factor/(drop**(epoch//time))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "I0s3-77cH6IQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model instance"
      ],
      "metadata": {
        "id": "opy9DAB8H7CQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, opt = get_registered_Transformer_model(in_vocab_size=ntokens, out_vocab_size=ntokens, dim_in = emsize, dim_key= emsize//nhead, heads = nhead, dim_internal=d_hid, copies = nlayers, lr = 1., start=starting_wrd, end = ending_wrd)\n",
        "model_operate = NN_operating_tools(model, opt, learning_rate_schedule= learning_rate_function(emsize, factor=2.5, warmup_steps = 8))"
      ],
      "metadata": {
        "id": "-ytMlh4tH-J4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data instance and untrained transformer output"
      ],
      "metadata": {
        "id": "orPbuONYJ-JK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "data = iter(dataloader)\n",
        "(x,y) = next(data)\n",
        "print(decode(x[0][0]))\n",
        "print(decode(x[1][0]))\n",
        "print(decode(y[0]))"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "IvtHx97hJ_LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "model.eval()\n",
        "out, _ = model(x)\n",
        "decode(out[0])"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "16BiSKaRcMsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "decode(model.autoregression(x[0][0].unsqueeze(0), 62, mode=\"greedy\"))"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "q9Dus9QFJ_wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "13HL2S2jKXVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_operate.fit_transformer(25, dataloader, tokenizer=tokenizer_out, sample_input=x[0][0].unsqueeze(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yYIi_HvPKQCD",
        "outputId": "8c4bde09-33fd-49a8-b25c-deb9faca5cbb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 -- time: 2.5308990478515625 -- loss = 139.3667755126953\n",
            "\n",
            "<start> friends friends friends for friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends\n",
            "Epoch: 1 -- time: 2.4642035961151123 -- loss = 139.05274963378906\n",
            "\n",
            "<start> friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends\n",
            "Epoch: 2 -- time: 2.8010153770446777 -- loss = 138.71156311035156\n",
            "\n",
            "<start> friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends\n",
            "Epoch: 3 -- time: 2.52878999710083 -- loss = 138.37161254882812\n",
            "\n",
            "<start> friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends\n",
            "Epoch: 4 -- time: 2.470517158508301 -- loss = 138.1101837158203\n",
            "\n",
            "<start> friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends\n",
            "Epoch: 5 -- time: 2.4618873596191406 -- loss = 137.80844116210938\n",
            "\n",
            "<start> friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends\n",
            "Epoch: 6 -- time: 2.7118279933929443 -- loss = 137.25535583496094\n",
            "\n",
            "<start> friends friends friends friends friends myself friends friends myself friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends friends myself friends friends friends friends friends friends friends friends friends friends myself friends myself friends myself friends\n",
            "Epoch: 7 -- time: 2.578585386276245 -- loss = 133.60813903808594\n",
            "\n",
            "<start> nobody young young young young young young myself myself myself myself myself myself myself myself myself myself nobody myself myself I've for myself myself myself myself myself myself I've, for myself myself myself myself myself myself myself myself myself myself myself myself myself I've, for myself myself myself myself myself myself myself myself myself myself myself myself myself myself myself\n",
            "Epoch: 8 -- time: 2.455479621887207 -- loss = 112.11148071289062\n",
            "\n",
            "<start> When was young I was young I was, I was young was young was just for, I was I was, I was, I was young I was I was young I was young I was young, I was I was just for, I nobody I nobody I nobody I nobody young, I myself! was\n",
            "Epoch: 9 -- time: 2.4613335132598877 -- loss = 78.35192108154297\n",
            "\n",
            "<start> When I was young, I never needed anyone and making love was just for fun. Those days are gone! Living myself, I think of all the friends I've known but when I dial the telephone nobody is home. All by myself, I don't wanna be all by myself,!!!\n",
            "Epoch: 10 -- time: 2.665698289871216 -- loss = 53.44459533691406\n",
            "\n",
            "<start> When I was young, I never needed anyone and making love was just for fun. Those days are gone! Living alone, I think of all the friends I've known but when I dial the telephone nobody is home. All by myself, I dial the friends I've known but when be all by myself\n",
            "Epoch: 11 -- time: 2.618361473083496 -- loss = 40.3934440612793\n",
            "\n",
            "<start> When I was young, I never needed anyone and making love was just for fun. Those days are gone! Living alone, I think of all the friends I've known but when I dial the telephone nobody is home. All by myself, I don't wanna be all by myself, anymore! <end>\n",
            "Epoch: 12 -- time: 2.450779676437378 -- loss = 35.0711784362793\n",
            "\n",
            "<start> When I was young, I never needed anyone and making love was just for fun. Those days are gone! Living alone, I think of all the friends I've known but when I dial the telephone nobody is home. All by myself, I don't wanna be all by myself, anymore! <end>\n",
            "Epoch: 13 -- time: 2.4541425704956055 -- loss = 31.723613739013672\n",
            "\n",
            "<start> When I was young, I never needed anyone and making love was just for fun. Those days are gone! Living alone, I think of all the friends I've known but when I dial the telephone nobody is home. All by myself, I don't wanna be all by myself, anymore! <end>\n",
            "Epoch: 14 -- time: 2.631129026412964 -- loss = 29.345172882080078\n",
            "\n",
            "<start> When I was young, I never needed anyone and making love was just for fun. Those days are gone! Living alone, I think of all the friends I've known but when I dial the telephone nobody is home. All by myself, I don't wanna be all by myself, anymore! <end>\n",
            "Epoch: 15 -- time: 2.7235100269317627 -- loss = 27.742145538330078\n",
            "\n",
            "<start> When I was young, I never needed anyone and making love was just for fun. Those days are gone! Living alone, I think of all the friends I've known but when I dial the telephone nobody is home. All by myself, I don't wanna be all by myself, anymore! <end>\n",
            "Epoch: 16 -- time: 3.144728183746338 -- loss = 26.44978141784668\n",
            "\n",
            "<start> When I was young, I never needed anyone and making love was just for fun. Those days are gone! Living alone, I think of all the friends I've known but when I dial the telephone nobody is home. All by myself, I don't wanna be all by myself, anymore! <end>\n",
            "Epoch: 17 -- time: 2.4608166217803955 -- loss = 25.4450626373291\n",
            "\n",
            "<start> When I was young, I never needed anyone and making love was just for fun. Those days are gone! Living alone, I think of all the friends I've known but when I dial the telephone nobody is home. All by myself, I don't wanna be all by myself, anymore! <end>\n",
            "Epoch: 18 -- time: 2.829944133758545 -- loss = 24.667009353637695\n",
            "\n",
            "<start> When I was young, I never needed anyone and making love was just for fun. Those days are gone! Living alone, I think of all the friends I've known but when I dial the telephone nobody is home. All by myself, I don't wanna be all by myself, anymore! <end>\n",
            "Epoch: 19 -- time: 2.5127532482147217 -- loss = 23.988187789916992\n",
            "\n",
            "<start> When I was young, I never needed anyone and making love was just for fun. Those days are gone! Living alone, I think of all the friends I've known but when I dial the telephone nobody is home. All by myself, I don't wanna be all by myself, anymore! <end>\n",
            "Epoch: 20 -- time: 2.4473485946655273 -- loss = 23.4418888092041\n",
            "\n",
            "<start> When I was young, I never needed anyone and making love was just for fun. Those days are gone! Living alone, I think of all the friends I've known but when I dial the telephone nobody is home. All by myself, I don't wanna be all by myself, anymore! <end>\n",
            "Epoch: 21 -- time: 2.4644601345062256 -- loss = 22.96198081970215\n",
            "\n",
            "<start> When I was young, I never needed anyone and making love was just for fun. Those days are gone! Living alone, I think of all the friends I've known but when I dial the telephone nobody is home. All by myself, I don't wanna be all by myself, anymore! <end>\n",
            "Epoch: 22 -- time: 2.7710330486297607 -- loss = 22.60419273376465\n",
            "\n",
            "<start> When I was young, I never needed anyone and making love was just for fun. Those days are gone! Living alone, I think of all the friends I've known but when I dial the telephone nobody is home. All by myself, I don't wanna be all by myself, anymore! <end>\n",
            "Epoch: 23 -- time: 2.518686056137085 -- loss = 22.24106216430664\n",
            "\n",
            "<start> When I was young, I never needed anyone and making love was just for fun. Those days are gone! Living alone, I think of all the friends I've known but when I dial the telephone nobody is home. All by myself, I don't wanna be all by myself, anymore! <end>\n",
            "Epoch: 24 -- time: 2.4583914279937744 -- loss = 21.993331909179688\n",
            "\n",
            "<start> When I was young, I never needed anyone and making love was just for fun. Those days are gone! Living alone, I think of all the friends I've known but when I dial the telephone nobody is home. All by myself, I don't wanna be all by myself, anymore! <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trained transformer autoregression output"
      ],
      "metadata": {
        "id": "8oURPi4eh37V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "model.eval()\n",
        "decode(model.autoregression(x[0][0].unsqueeze(0), 62, mode=\"beam\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "GooXu1bAKmPL",
        "outputId": "f462164e-a471-426f-fb1e-a134878a13bd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<start> When I was young, I never needed anyone and making love was just for fun. Those days are gone! Living alone, I think of all the friends I've known but when I dial the telephone nobody is home. All by myself, I don't wanna be all by myself, anymore! <end>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out, _ =model(x)\n",
        "decode(out[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "pYNi9GofhUtB",
        "outputId": "67cf6a01-429c-479d-d7ae-abfa511f3ff8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"When I was young, I never needed anyone and making love was just for fun. Those days are gone! Living alone, I think of all the friends I've known but when I dial the telephone nobody is home. All by myself, I don't wanna be all by myself, anymore! <end>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_operate.save_pt(\"small_trained_transformer.pt\")"
      ],
      "metadata": {
        "id": "kT14ZeJJKTVV"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}