{"cells":[{"cell_type":"markdown","metadata":{"id":"DZ-2Q5H1q5zf"},"source":["# Importing NN modules and data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21449,"status":"ok","timestamp":1677143262483,"user":{"displayName":"Lampros Lamprou","userId":"14259265734214740857"},"user_tz":480},"id":"TrXnD9_1qquw","outputId":"387d0096-6896-4b59-c422-8b5fdcdb6ef5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E5uiN3mWqzNu"},"outputs":[],"source":["import sys\n","sys.path.insert(0,\"/content/drive/My Drive/Colab Notebooks/NeuralNetwork\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BHdsAF_sq3B8"},"outputs":[],"source":["import math\n","import os\n","from tempfile import TemporaryDirectory\n","from typing import Tuple\n","\n","import torch\n","from torch import nn, Tensor\n","import torch.nn.functional as F\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","from torch.utils.data import dataset\n","\n","import numpy as np\n","import random\n","import copy\n","import torch.optim as optim\n","import pickle\n","import os\n","import pandas as pd\n","import time\n","\n","from library_model import layers as lay\n","from library_model import model_building as mb\n","from library_model import model_training as mt\n","from data import data_loading as dt\n","from data import text as txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":5755,"status":"ok","timestamp":1677143274614,"user":{"displayName":"Lampros Lamprou","userId":"14259265734214740857"},"user_tz":480},"id":"FCqHvjEPtg-4","outputId":"19b7f1e1-c852-4acd-a702-6c6de5202f46"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchdata\n","  Downloading torchdata-0.5.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.6/4.6 MB 76.9 MB/s eta 0:00:00\n","Collecting portalocker>=2.0.0\n","  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchdata) (2.25.1)\n","Collecting urllib3>=1.25\n","  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 KB 19.2 MB/s eta 0:00:00\n","Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchdata) (1.13.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->torchdata) (4.5.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (4.0.0)\n","Installing collected packages: urllib3, portalocker, torchdata\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed portalocker-2.7.0 torchdata-0.5.1 urllib3-1.26.14\n"]}],"source":["%%bash\n","pip install torchdata"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":6336,"status":"ok","timestamp":1677143285039,"user":{"displayName":"Lampros Lamprou","userId":"14259265734214740857"},"user_tz":480},"id":"gy0ntXOnt8C3","outputId":"a0308a3b-10ad-489b-ad29-a1c286b7c633"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchtext in /usr/local/lib/python3.8/dist-packages (0.14.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext) (2.25.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext) (1.22.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext) (4.64.1)\n","Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchtext) (1.13.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->torchtext) (4.5.0)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (1.26.14)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (2022.12.7)\n"]}],"source":["%%bash\n","pip install torchtext"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AEhPPzhMrEz9"},"outputs":[],"source":["from torchtext.datasets import WikiText2\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","train_iter = WikiText2(split='train')"]},{"cell_type":"markdown","metadata":{"id":"HxQV9yNqzjz6"},"source":["# Define Network State"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z3zHFEMHrO4q"},"outputs":[],"source":["state = lay.Network_state()\n","p= state.parameters\n","tr= state.training\n","\n","p.d_model=200\n","p.d_hid=200\n","p.nheads=2\n","p.d_key = p.d_model //p.nheads\n","p.nlayers=2\n","\n","\n","tr.batch_size=20\n","tr.lr = 1.\n","tr.seq_length=35\n","tr.optimizer = \"sgd\"\n","tr.schedule = mb.learning_rate_step(5., 20/19, 1)"]},{"cell_type":"markdown","metadata":{"id":"oC9zXMd5rQGT"},"source":["# Process Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-LzgbKzc0Ode"},"outputs":[],"source":["d = dt.Data()\n","d.tokenizer, d.train_dataloader = dt.process_data(train_iter, txt.library_text_coders, state)\n","p.ntokens = p.ntokens_out = len(d.tokenizer.vocab)"]},{"cell_type":"markdown","metadata":{"id":"-VniQvGhpshX"},"source":["Data instance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1677145738875,"user":{"displayName":"Lampros Lamprou","userId":"14259265734214740857"},"user_tz":480},"id":"Q29CuLM7pvQw","outputId":"872e3723-f8e5-4706-f6da-415a73108a98"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'number of promotional adverts were aired on seven . the storyline began on @-@ screen when nicole started dating elliot <unk> ( paul <unk> ) . he had a <unk> against roman and kidnapped'"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["([a], b) = next(iter(d.train_dataloader))\n","d.decode(a[5])"]},{"cell_type":"markdown","metadata":{"id":"WEQGJEdvrHcJ"},"source":["# Construct Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WIxtv-t8rgBL"},"outputs":[],"source":["parts = mb.get_transformer_parts(state)\n","model = lay.EncoderModel(parts.encoder, parts.linear).to(state.device)\n","#transformer = lay.Transformer(parts.encoder, parts.decoder, parts.linear).to(state.device)"]},{"cell_type":"markdown","metadata":{"id":"RQL3KbrGrXwf"},"source":["# Train Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"De0JgI7LrT7i"},"outputs":[],"source":["opt, scheduler = mb.get_optimizer(state, model)\n","criterion = nn.CrossEntropyLoss()\n","train = mt.Model_training(model, opt, criterion, scheduler)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rTSBfGPA0Odf"},"outputs":[],"source":["train(epochs = 3, data = d)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mMcNafbXjDSa"},"outputs":[],"source":["def train2(model: nn.Module) -> None:\n","    model.train()  # turn on train mode\n","    total_loss = 0.\n","    log_interval = 200\n","    start_time = time.time()\n","\n","    batch =0\n","    training_data = iter(d.train_dataloader)\n","    for [data], targets in training_data: \n","        output = model(data)\n","        loss = criterion(output.view(-1, p.ntokens), targets.reshape(-1))\n","\n","        opt.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n","        opt.step()\n","\n","        total_loss += loss.item()\n","        if batch % log_interval == 0 and batch > 0:\n","            lr = scheduler.get_last_lr()[0]\n","            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n","            cur_loss = total_loss / log_interval\n","            ppl = math.exp(cur_loss)\n","            print(f'| epoch {epoch:3d} | {batch:5d} batches | '\n","                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n","                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n","            total_loss = 0\n","            start_time = time.time()\n","        batch +=1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":141906,"status":"ok","timestamp":1677145926894,"user":{"displayName":"Lampros Lamprou","userId":"14259265734214740857"},"user_tz":480},"id":"3PKTzL2Mjw3y","outputId":"08a3ff45-b1bc-4b6f-b16a-d95a395fa635"},"outputs":[{"name":"stdout","output_type":"stream","text":["| epoch   1 |   200 batches | lr 5.00 | ms/batch 16.37 | loss  8.74 | ppl  6223.59\n","| epoch   1 |   400 batches | lr 5.00 | ms/batch 16.42 | loss  7.77 | ppl  2367.57\n","| epoch   1 |   600 batches | lr 5.00 | ms/batch 16.95 | loss  7.52 | ppl  1843.31\n","| epoch   1 |   800 batches | lr 5.00 | ms/batch 16.05 | loss  7.35 | ppl  1563.16\n","| epoch   1 |  1000 batches | lr 5.00 | ms/batch 16.08 | loss  7.20 | ppl  1334.62\n","| epoch   1 |  1200 batches | lr 5.00 | ms/batch 16.23 | loss  7.09 | ppl  1197.01\n","| epoch   1 |  1400 batches | lr 5.00 | ms/batch 16.96 | loss  7.03 | ppl  1124.58\n","| epoch   1 |  1600 batches | lr 5.00 | ms/batch 15.92 | loss  6.94 | ppl  1037.27\n","| epoch   1 |  1800 batches | lr 5.00 | ms/batch 15.92 | loss  6.91 | ppl  1000.96\n","| epoch   1 |  2000 batches | lr 5.00 | ms/batch 15.81 | loss  6.88 | ppl   969.13\n","| epoch   1 |  2200 batches | lr 5.00 | ms/batch 16.88 | loss  6.84 | ppl   930.87\n","| epoch   1 |  2400 batches | lr 5.00 | ms/batch 15.94 | loss  6.85 | ppl   940.00\n","| epoch   1 |  2600 batches | lr 5.00 | ms/batch 15.73 | loss  6.77 | ppl   868.05\n","| epoch   1 |  2800 batches | lr 5.00 | ms/batch 15.74 | loss  6.78 | ppl   878.99\n","| epoch   2 |   200 batches | lr 4.75 | ms/batch 16.77 | loss  6.66 | ppl   779.68\n","| epoch   2 |   400 batches | lr 4.75 | ms/batch 15.58 | loss  6.64 | ppl   766.62\n","| epoch   2 |   600 batches | lr 4.75 | ms/batch 15.66 | loss  6.63 | ppl   754.28\n","| epoch   2 |   800 batches | lr 4.75 | ms/batch 15.62 | loss  6.60 | ppl   735.66\n","| epoch   2 |  1000 batches | lr 4.75 | ms/batch 16.90 | loss  6.60 | ppl   733.86\n","| epoch   2 |  1200 batches | lr 4.75 | ms/batch 15.85 | loss  6.57 | ppl   711.29\n","| epoch   2 |  1400 batches | lr 4.75 | ms/batch 15.72 | loss  6.53 | ppl   684.45\n","| epoch   2 |  1600 batches | lr 4.75 | ms/batch 15.67 | loss  6.55 | ppl   696.94\n","| epoch   2 |  1800 batches | lr 4.75 | ms/batch 16.52 | loss  6.56 | ppl   705.22\n","| epoch   2 |  2000 batches | lr 4.75 | ms/batch 16.19 | loss  6.50 | ppl   663.51\n","| epoch   2 |  2200 batches | lr 4.75 | ms/batch 15.81 | loss  6.52 | ppl   677.61\n","| epoch   2 |  2400 batches | lr 4.75 | ms/batch 15.75 | loss  6.49 | ppl   661.17\n","| epoch   2 |  2600 batches | lr 4.75 | ms/batch 16.34 | loss  6.49 | ppl   657.09\n","| epoch   2 |  2800 batches | lr 4.75 | ms/batch 16.66 | loss  6.44 | ppl   627.95\n","| epoch   3 |   200 batches | lr 4.51 | ms/batch 15.88 | loss  6.40 | ppl   601.31\n","| epoch   3 |   400 batches | lr 4.51 | ms/batch 15.81 | loss  6.37 | ppl   583.81\n","| epoch   3 |   600 batches | lr 4.51 | ms/batch 16.80 | loss  6.40 | ppl   602.06\n","| epoch   3 |   800 batches | lr 4.51 | ms/batch 16.05 | loss  6.42 | ppl   613.19\n","| epoch   3 |  1000 batches | lr 4.51 | ms/batch 15.74 | loss  6.37 | ppl   581.19\n","| epoch   3 |  1200 batches | lr 4.51 | ms/batch 15.80 | loss  6.37 | ppl   581.75\n","| epoch   3 |  1400 batches | lr 4.51 | ms/batch 16.41 | loss  6.33 | ppl   561.86\n","| epoch   3 |  1600 batches | lr 4.51 | ms/batch 16.25 | loss  6.33 | ppl   559.06\n","| epoch   3 |  1800 batches | lr 4.51 | ms/batch 15.73 | loss  6.34 | ppl   564.20\n","| epoch   3 |  2000 batches | lr 4.51 | ms/batch 15.68 | loss  6.38 | ppl   592.55\n","| epoch   3 |  2200 batches | lr 4.51 | ms/batch 16.14 | loss  6.35 | ppl   571.17\n","| epoch   3 |  2400 batches | lr 4.51 | ms/batch 16.72 | loss  6.31 | ppl   552.19\n","| epoch   3 |  2600 batches | lr 4.51 | ms/batch 15.64 | loss  6.32 | ppl   556.88\n","| epoch   3 |  2800 batches | lr 4.51 | ms/batch 15.63 | loss  6.30 | ppl   544.59\n"]}],"source":["epochs = 3\n","for epoch in range(1, epochs + 1):\n","  train2(model)\n","  scheduler.step()"]},{"cell_type":"markdown","metadata":{"id":"3Vm_ljhwjE-m"},"source":["# Pytorch tutorial model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O5y8W6dL8knG"},"outputs":[],"source":["#@title\n","class TransformerModel(nn.Module):\n","\n","    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n","                 nlayers: int, dropout: float = 0.5):\n","        super().__init__()\n","        self.model_type = 'Transformer'\n","        self.pos_encoder = PositionalEncoding(d_model, dropout)\n","        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n","        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n","        self.encoder = nn.Embedding(ntoken, d_model)\n","        self.d_model = d_model\n","        self.decoder = nn.Linear(d_model, ntoken)\n","\n","        self.init_weights()\n","\n","    def init_weights(self) -> None:\n","        initrange = 0.1\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.bias.data.zero_()\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src: Tensor, src_mask) -> Tensor:\n","        \"\"\"\n","        Args:\n","            src: Tensor, shape [seq_len, batch_size]\n","            src_mask: Tensor, shape [seq_len, seq_len]\n","\n","        Returns:\n","            output Tensor of shape [seq_len, batch_size, ntoken]\n","        \"\"\"\n","        src = self.encoder(src) * math.sqrt(self.d_model)\n","        src = self.pos_encoder(src)\n","        output = self.transformer_encoder(src, src_mask)\n","        output = self.decoder(output)\n","        return output\n","\n","\n","def generate_square_subsequent_mask(sz: int) -> Tensor:\n","    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n","    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(torch.device(\"cuda\"))\n","\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe = torch.zeros(max_len, 1, d_model)\n","        pe[:, 0, 0::2] = torch.sin(position * div_term)\n","        pe[:, 0, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        \"\"\"\n","        Args:\n","            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n","        \"\"\"\n","        x = x + self.pe[:x.size(0)]\n","        return self.dropout(x).to(torch.device(\"cuda\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbp0X2NT9Jle"},"outputs":[],"source":["tutorial_model = TransformerModel(p.ntokens, p.d_model,p.nheads, p.d_hid, p.nlayers, dropout=0.2).to(state.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MitJy8omJ0jn"},"outputs":[],"source":["train_iter = WikiText2(split='train')\n","tokenizer = get_tokenizer('basic_english')\n","vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n","vocab.set_default_index(vocab['<unk>'])\n","\n","def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n","    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n","    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n","    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n","\n","# train_iter was \"consumed\" by the process of building the vocab,\n","# so we have to create it again\n","train_iter, val_iter, test_iter = WikiText2()\n","train_data = data_process(train_iter)\n","val_data = data_process(val_iter)\n","test_data = data_process(test_iter)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def batchify(data: Tensor, bsz: int) -> Tensor:\n","    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n","    that wouldn't cleanly fit.\n","\n","    Args:\n","        data: Tensor, shape [N]\n","        bsz: int, batch size\n","\n","    Returns:\n","        Tensor of shape [N // bsz, bsz]\n","    \"\"\"\n","    seq_len = data.size(0) // bsz\n","    data = data[:seq_len * bsz]\n","    data = data.view(bsz, seq_len).t().contiguous()\n","    return data.to(device)\n","\n","batch_size = 20\n","eval_batch_size = 10\n","train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n","val_data = batchify(val_data, eval_batch_size)\n","test_data = batchify(test_data, eval_batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w1WWRmhvJ62i"},"outputs":[],"source":["bptt = 35\n","def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n","    \"\"\"\n","    Args:\n","        source: Tensor, shape [full_seq_len, batch_size]\n","        i: int\n","\n","    Returns:\n","        tuple (data, target), where data has shape [seq_len, batch_size] and\n","        target has shape [seq_len * batch_size]\n","    \"\"\"\n","    seq_len = min(bptt, len(source) - 1 - i)\n","    data = source[i:i+seq_len].transpose(0,1)\n","    target = source[i+1:i+1+seq_len].transpose(0,1).reshape(-1)\n","    return data, target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BEwWoi3v_IH9"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","lr = 5.0  # learning rate\n","optimizer2 = torch.optim.SGD(tutorial_model.parameters(), lr=lr)\n","scheduler2 = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n","bptt = 35\n","\n","def train(model: nn.Module) -> None:\n","    model.train()  # turn on train mode\n","    total_loss = 0.\n","    log_interval = 200\n","    start_time = time.time()\n","\n","    num_batches = len(train_data) // bptt\n","    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n","        data, targets = get_batch(train_data, i)\n","        seq_len = data.size(1)\n","        src_mask = generate_square_subsequent_mask(seq_len)\n","        output = model(data, src_mask)\n","        loss = criterion(output.view(-1, p.ntokens), targets)\n","\n","        optimizer2.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n","        optimizer2.step()\n","\n","        total_loss += loss.item()\n","        if batch % log_interval == 0 and batch > 0:\n","            lr = scheduler2.get_last_lr()[0]\n","            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n","            cur_loss = total_loss / log_interval\n","            ppl = math.exp(cur_loss)\n","            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n","                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n","                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n","            total_loss = 0\n","            start_time = time.time()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":132606,"status":"ok","timestamp":1677145199235,"user":{"displayName":"Lampros Lamprou","userId":"14259265734214740857"},"user_tz":480},"id":"P_dCA7fvjpGx","outputId":"663d7a77-9daa-4032-d165-f3d88cd87679"},"outputs":[{"name":"stdout","output_type":"stream","text":["| epoch   1 |   200/ 2928 batches | lr 4.29 | ms/batch 15.76 | loss  8.16 | ppl  3509.78\n","| epoch   1 |   400/ 2928 batches | lr 4.29 | ms/batch 14.94 | loss  6.88 | ppl   975.42\n","| epoch   1 |   600/ 2928 batches | lr 4.29 | ms/batch 14.88 | loss  6.41 | ppl   608.47\n","| epoch   1 |   800/ 2928 batches | lr 4.29 | ms/batch 14.94 | loss  6.25 | ppl   518.87\n","| epoch   1 |  1000/ 2928 batches | lr 4.29 | ms/batch 15.20 | loss  6.15 | ppl   470.09\n","| epoch   1 |  1200/ 2928 batches | lr 4.29 | ms/batch 15.41 | loss  6.11 | ppl   451.18\n","| epoch   1 |  1400/ 2928 batches | lr 4.29 | ms/batch 16.51 | loss  6.09 | ppl   441.12\n","| epoch   1 |  1600/ 2928 batches | lr 4.29 | ms/batch 15.07 | loss  6.07 | ppl   432.16\n","| epoch   1 |  1800/ 2928 batches | lr 4.29 | ms/batch 15.21 | loss  5.98 | ppl   396.23\n","| epoch   1 |  2000/ 2928 batches | lr 4.29 | ms/batch 15.63 | loss  5.97 | ppl   391.47\n","| epoch   1 |  2200/ 2928 batches | lr 4.29 | ms/batch 16.04 | loss  5.86 | ppl   351.62\n","| epoch   1 |  2400/ 2928 batches | lr 4.29 | ms/batch 15.04 | loss  5.93 | ppl   375.81\n","| epoch   1 |  2600/ 2928 batches | lr 4.29 | ms/batch 14.95 | loss  5.92 | ppl   372.13\n","| epoch   1 |  2800/ 2928 batches | lr 4.29 | ms/batch 15.31 | loss  5.84 | ppl   344.34\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"]},{"name":"stdout","output_type":"stream","text":["| epoch   2 |   200/ 2928 batches | lr 4.07 | ms/batch 14.97 | loss  5.84 | ppl   345.15\n","| epoch   2 |   400/ 2928 batches | lr 4.07 | ms/batch 14.82 | loss  5.80 | ppl   331.88\n","| epoch   2 |   600/ 2928 batches | lr 4.07 | ms/batch 14.84 | loss  5.58 | ppl   266.13\n","| epoch   2 |   800/ 2928 batches | lr 4.07 | ms/batch 15.27 | loss  5.60 | ppl   270.89\n","| epoch   2 |  1000/ 2928 batches | lr 4.07 | ms/batch 15.03 | loss  5.56 | ppl   258.95\n","| epoch   2 |  1200/ 2928 batches | lr 4.07 | ms/batch 14.76 | loss  5.60 | ppl   269.39\n","| epoch   2 |  1400/ 2928 batches | lr 4.07 | ms/batch 14.79 | loss  5.62 | ppl   275.48\n","| epoch   2 |  1600/ 2928 batches | lr 4.07 | ms/batch 14.92 | loss  5.65 | ppl   284.40\n","| epoch   2 |  1800/ 2928 batches | lr 4.07 | ms/batch 15.28 | loss  5.59 | ppl   266.66\n","| epoch   2 |  2000/ 2928 batches | lr 4.07 | ms/batch 14.78 | loss  5.59 | ppl   266.60\n","| epoch   2 |  2200/ 2928 batches | lr 4.07 | ms/batch 14.77 | loss  5.49 | ppl   242.56\n","| epoch   2 |  2400/ 2928 batches | lr 4.07 | ms/batch 14.81 | loss  5.58 | ppl   263.81\n","| epoch   2 |  2600/ 2928 batches | lr 4.07 | ms/batch 15.18 | loss  5.59 | ppl   268.86\n","| epoch   2 |  2800/ 2928 batches | lr 4.07 | ms/batch 15.14 | loss  5.53 | ppl   251.76\n","| epoch   3 |   200/ 2928 batches | lr 3.87 | ms/batch 14.83 | loss  5.56 | ppl   260.86\n","| epoch   3 |   400/ 2928 batches | lr 3.87 | ms/batch 14.86 | loss  5.56 | ppl   260.08\n","| epoch   3 |   600/ 2928 batches | lr 3.87 | ms/batch 15.31 | loss  5.34 | ppl   208.53\n","| epoch   3 |   800/ 2928 batches | lr 3.87 | ms/batch 15.21 | loss  5.37 | ppl   215.72\n","| epoch   3 |  1000/ 2928 batches | lr 3.87 | ms/batch 14.89 | loss  5.34 | ppl   208.65\n","| epoch   3 |  1200/ 2928 batches | lr 3.87 | ms/batch 14.84 | loss  5.39 | ppl   218.61\n","| epoch   3 |  1400/ 2928 batches | lr 3.87 | ms/batch 14.97 | loss  5.40 | ppl   222.11\n","| epoch   3 |  1600/ 2928 batches | lr 3.87 | ms/batch 15.41 | loss  5.45 | ppl   232.07\n","| epoch   3 |  1800/ 2928 batches | lr 3.87 | ms/batch 14.89 | loss  5.38 | ppl   218.01\n","| epoch   3 |  2000/ 2928 batches | lr 3.87 | ms/batch 14.89 | loss  5.39 | ppl   219.94\n","| epoch   3 |  2200/ 2928 batches | lr 3.87 | ms/batch 14.89 | loss  5.29 | ppl   198.36\n","| epoch   3 |  2400/ 2928 batches | lr 3.87 | ms/batch 15.26 | loss  5.39 | ppl   219.97\n","| epoch   3 |  2600/ 2928 batches | lr 3.87 | ms/batch 15.25 | loss  5.41 | ppl   223.24\n","| epoch   3 |  2800/ 2928 batches | lr 3.87 | ms/batch 14.90 | loss  5.35 | ppl   210.41\n"]}],"source":["for epoch in range(1, epochs + 1):\n","        epoch_start_time = time.time()\n","        train(tutorial_model)\n","        scheduler2.step()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["3Vm_ljhwjE-m"],"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.7.6 ('base')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.6"},"vscode":{"interpreter":{"hash":"8f1af38ef70d2ba974426d7a8a6b596b940002c76a66dd7cd7f620996d04d624"}}},"nbformat":4,"nbformat_minor":0}
