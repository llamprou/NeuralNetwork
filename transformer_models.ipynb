{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ-2Q5H1q5zf"
      },
      "source": [
        "# Importing NN modules and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrXnD9_1qquw",
        "outputId": "31877429-a788-4e37-ee53-f12bd067ff26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E5uiN3mWqzNu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0,\"/content/drive/My Drive/Colab Notebooks/NeuralNetwork\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BHdsAF_sq3B8"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from library_model import layers as lay\n",
        "from library_model import model_building as mb\n",
        "from library_model import model_training as mt\n",
        "from data import data_loading as dt\n",
        "from data import text as txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FCqHvjEPtg-4",
        "outputId": "87fcc4ff-e451-4723-b3b7-f57235198ddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.5.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.6/4.6 MB 54.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.8/dist-packages (from torchdata) (1.26.14)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchdata) (2.25.1)\n",
            "Collecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchdata) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->torchdata) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (2.10)\n",
            "Installing collected packages: portalocker, torchdata\n",
            "Successfully installed portalocker-2.7.0 torchdata-0.5.1\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install torchdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gy0ntXOnt8C3",
        "outputId": "daf10cea-9528-4a7d-be43-cc027f6855d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.8/dist-packages (0.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext) (2.25.1)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchtext) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->torchtext) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (4.0.0)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AEhPPzhMrEz9"
      },
      "outputs": [],
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_iter = WikiText2(split='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxQV9yNqzjz6"
      },
      "source": [
        "# Define Network State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Z3zHFEMHrO4q"
      },
      "outputs": [],
      "source": [
        "state = lay.Network_state()\n",
        "p= state.parameters\n",
        "tr= state.training\n",
        "\n",
        "p.d_model=600\n",
        "p.d_hid=600\n",
        "p.nheads=4\n",
        "p.d_key = p.d_model //p.nheads\n",
        "p.nlayers=2\n",
        "\n",
        "tr.num_epochs = 6\n",
        "tr.batch_size=20\n",
        "tr.data_fraction = 0.8\n",
        "tr.lr = 1.\n",
        "tr.seq_length=35\n",
        "tr.optimizer = \"sgd\"\n",
        "tr.schedule = mb.learning_rate_cosine(factor = 5, length = 2300*tr.num_epochs, finalLR=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC9zXMd5rQGT"
      },
      "source": [
        "# Process Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-LzgbKzc0Ode"
      },
      "outputs": [],
      "source": [
        "d = dt.Data()\n",
        "d.tokenizer, d.train_dataloader, d.test_dataloader = dt.process_data(train_iter, txt.library_text_coders, state, network = \"encoder\") #switch to transformer if interested in encoder/decoder model\n",
        "p.ntokens = p.ntokens_out = len(d.tokenizer.vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VniQvGhpshX"
      },
      "source": [
        "Data instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Q29CuLM7pvQw",
        "outputId": "967ee357-c19e-470c-bb8f-ec1e3fbfabff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'babaorum village . tintin <unk> them , and the m <unk> cease hostilities and come to <unk> tintin . muganga and the stowaway plot to kill tintin and make it look like a leopard'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#train\n",
        "([a1], b1) = next(iter(d.train_dataloader))\n",
        "d.decode(a1[5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "([a2], b2) = next(iter(d.test_dataloader))\n",
        "d.decode(a2[5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "s6NbkM_wsYWN",
        "outputId": "ff7ba84e-d93d-4491-c324-0a721e2cf53f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'framework is a meteorologist who predicts the weather . focusing on the social <unk> , goffman seeks to construct a general statement regarding the structure , or form , of experiences individuals have at'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEQGJEdvrHcJ"
      },
      "source": [
        "# Construct Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WIxtv-t8rgBL"
      },
      "outputs": [],
      "source": [
        "parts = mb.get_transformer_parts(state)\n",
        "model = lay.EncoderModel(parts.encoder, parts.linear).to(state.device)\n",
        "#transformer = lay.Transformer(parts.encoder, parts.decoder, parts.linear).to(state.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQL3KbrGrXwf"
      },
      "source": [
        "# Train Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "De0JgI7LrT7i"
      },
      "outputs": [],
      "source": [
        "opt, scheduler = mb.get_optimizer(state, model)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train = mt.Model_training(model, opt, criterion, scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rTSBfGPA0Odf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f1846ab4-144e-4285-b7a3-c80dad99cb7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | lr = 5.497 | time =  0.18 | train_loss 10.67\n",
            "Batch 400 | lr = 5.490 | time =  0.19 | train_loss  7.94\n",
            "Batch 600 | lr = 5.477 | time =  0.19 | train_loss  7.56\n",
            "Batch 800 | lr = 5.459 | time =  0.20 | train_loss  7.32\n",
            "Batch 1000 | lr = 5.436 | time =  0.21 | train_loss  7.16\n",
            "Batch 1200 | lr = 5.408 | time =  0.20 | train_loss  7.04\n",
            "Batch 1400 | lr = 5.374 | time =  0.19 | train_loss  6.99\n",
            "Batch 1600 | lr = 5.336 | time =  0.18 | train_loss  6.88\n",
            "Batch 1800 | lr = 5.293 | time =  0.21 | train_loss  6.79\n",
            "Epoch: 0 | time: 95.98 | test_loss =  6.99\n",
            "\n",
            "Batch 200 | lr = 5.227 | time =  0.20 | train_loss  6.75\n",
            "Batch 400 | lr = 5.173 | time =  0.19 | train_loss  6.70\n",
            "Batch 600 | lr = 5.115 | time =  0.20 | train_loss  6.64\n",
            "Batch 800 | lr = 5.052 | time =  0.20 | train_loss  6.63\n",
            "Batch 1000 | lr = 4.985 | time =  0.21 | train_loss  6.54\n",
            "Batch 1200 | lr = 4.913 | time =  0.20 | train_loss  6.53\n",
            "Batch 1400 | lr = 4.838 | time =  0.19 | train_loss  6.50\n",
            "Batch 1600 | lr = 4.759 | time =  0.20 | train_loss  6.50\n",
            "Batch 1800 | lr = 4.677 | time =  0.19 | train_loss  6.46\n",
            "Epoch: 1 | time: 96.02 | test_loss =  6.49\n",
            "\n",
            "Batch 200 | lr = 4.558 | time =  0.18 | train_loss  6.37\n",
            "Batch 400 | lr = 4.467 | time =  0.20 | train_loss  6.37\n",
            "Batch 600 | lr = 4.374 | time =  0.20 | train_loss  6.30\n",
            "Batch 800 | lr = 4.277 | time =  0.20 | train_loss  6.30\n",
            "Batch 1000 | lr = 4.178 | time =  0.18 | train_loss  6.29\n",
            "Batch 1200 | lr = 4.077 | time =  0.20 | train_loss  6.27\n",
            "Batch 1400 | lr = 3.973 | time =  0.20 | train_loss  6.22\n",
            "Batch 1600 | lr = 3.867 | time =  0.20 | train_loss  6.24\n",
            "Batch 1800 | lr = 3.759 | time =  0.19 | train_loss  6.19\n",
            "Epoch: 2 | time: 96.19 | test_loss =  6.31\n",
            "\n",
            "Batch 200 | lr = 3.610 | time =  0.20 | train_loss  6.14\n",
            "Batch 400 | lr = 3.499 | time =  0.20 | train_loss  6.10\n",
            "Batch 600 | lr = 3.387 | time =  0.20 | train_loss  6.08\n",
            "Batch 800 | lr = 3.274 | time =  0.20 | train_loss  6.07\n",
            "Batch 1000 | lr = 3.161 | time =  0.21 | train_loss  6.05\n",
            "Batch 1200 | lr = 3.047 | time =  0.20 | train_loss  6.04\n",
            "Batch 1400 | lr = 2.933 | time =  0.21 | train_loss  6.00\n",
            "Batch 1600 | lr = 2.820 | time =  0.23 | train_loss  6.01\n",
            "Batch 1800 | lr = 2.706 | time =  0.18 | train_loss  5.99\n",
            "Epoch: 3 | time: 95.82 | test_loss =  6.14\n",
            "\n",
            "Batch 200 | lr = 2.552 | time =  0.20 | train_loss  5.89\n",
            "Batch 400 | lr = 2.441 | time =  0.20 | train_loss  5.93\n",
            "Batch 600 | lr = 2.330 | time =  0.20 | train_loss  5.90\n",
            "Batch 800 | lr = 2.222 | time =  0.19 | train_loss  5.90\n",
            "Batch 1000 | lr = 2.114 | time =  0.20 | train_loss  5.88\n",
            "Batch 1200 | lr = 2.009 | time =  0.20 | train_loss  5.83\n",
            "Batch 1400 | lr = 1.906 | time =  0.20 | train_loss  5.84\n",
            "Batch 1600 | lr = 1.804 | time =  0.20 | train_loss  5.83\n",
            "Batch 1800 | lr = 1.706 | time =  0.20 | train_loss  5.85\n",
            "Epoch: 4 | time: 96.44 | test_loss =  6.04\n",
            "\n",
            "Batch 200 | lr = 1.575 | time =  0.20 | train_loss  5.76\n",
            "Batch 400 | lr = 1.483 | time =  0.20 | train_loss  5.77\n",
            "Batch 600 | lr = 1.394 | time =  0.20 | train_loss  5.75\n",
            "Batch 800 | lr = 1.309 | time =  0.20 | train_loss  5.74\n",
            "Batch 1000 | lr = 1.227 | time =  0.20 | train_loss  5.73\n",
            "Batch 1200 | lr = 1.148 | time =  0.19 | train_loss  5.73\n",
            "Batch 1400 | lr = 1.074 | time =  0.19 | train_loss  5.72\n",
            "Batch 1600 | lr = 1.003 | time =  0.20 | train_loss  5.68\n",
            "Batch 1800 | lr = 0.937 | time =  0.20 | train_loss  5.69\n",
            "Epoch: 5 | time: 96.92 | test_loss =  5.95\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x216 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "train(epochs = tr.num_epochs, data = d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vm_ljhwjE-m"
      },
      "source": [
        "# Pytorch tutorial model  (code copied from pytorch tutorial for comparison)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5y8W6dL8knG"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [seq_len, batch_size]\n",
        "            src_mask: Tensor, shape [seq_len, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
        "        \"\"\"\n",
        "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
        "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(torch.device(\"cuda\"))\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x).to(torch.device(\"cuda\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbp0X2NT9Jle"
      },
      "outputs": [],
      "source": [
        "tutorial_model = TransformerModel(p.ntokens, p.d_model,p.nheads, p.d_hid, p.nlayers, dropout=0.2).to(state.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MitJy8omJ0jn"
      },
      "outputs": [],
      "source": [
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "# train_iter was \"consumed\" by the process of building the vocab,\n",
        "# so we have to create it again\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1WWRmhvJ62i"
      },
      "outputs": [],
      "source": [
        "bptt = 35\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len].transpose(0,1)\n",
        "    target = source[i+1:i+1+seq_len].transpose(0,1).reshape(-1)\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEwWoi3v_IH9"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # learning rate\n",
        "optimizer2 = torch.optim.SGD(tutorial_model.parameters(), lr=lr)\n",
        "scheduler2 = torch.optim.lr_scheduler.StepLR(optimizer2, 1.0, gamma=0.95)\n",
        "bptt = 35\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "\n",
        "    num_batches = len(train_data) // bptt\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        seq_len = data.size(1)\n",
        "        src_mask = generate_square_subsequent_mask(seq_len)\n",
        "        output = model(data, src_mask)\n",
        "        loss = criterion(output.view(-1, p.ntokens), targets)\n",
        "\n",
        "        optimizer2.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer2.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler2.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_dCA7fvjpGx",
        "outputId": "663d7a77-9daa-4032-d165-f3d88cd87679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 2928 batches | lr 4.29 | ms/batch 15.76 | loss  8.16 | ppl  3509.78\n",
            "| epoch   1 |   400/ 2928 batches | lr 4.29 | ms/batch 14.94 | loss  6.88 | ppl   975.42\n",
            "| epoch   1 |   600/ 2928 batches | lr 4.29 | ms/batch 14.88 | loss  6.41 | ppl   608.47\n",
            "| epoch   1 |   800/ 2928 batches | lr 4.29 | ms/batch 14.94 | loss  6.25 | ppl   518.87\n",
            "| epoch   1 |  1000/ 2928 batches | lr 4.29 | ms/batch 15.20 | loss  6.15 | ppl   470.09\n",
            "| epoch   1 |  1200/ 2928 batches | lr 4.29 | ms/batch 15.41 | loss  6.11 | ppl   451.18\n",
            "| epoch   1 |  1400/ 2928 batches | lr 4.29 | ms/batch 16.51 | loss  6.09 | ppl   441.12\n",
            "| epoch   1 |  1600/ 2928 batches | lr 4.29 | ms/batch 15.07 | loss  6.07 | ppl   432.16\n",
            "| epoch   1 |  1800/ 2928 batches | lr 4.29 | ms/batch 15.21 | loss  5.98 | ppl   396.23\n",
            "| epoch   1 |  2000/ 2928 batches | lr 4.29 | ms/batch 15.63 | loss  5.97 | ppl   391.47\n",
            "| epoch   1 |  2200/ 2928 batches | lr 4.29 | ms/batch 16.04 | loss  5.86 | ppl   351.62\n",
            "| epoch   1 |  2400/ 2928 batches | lr 4.29 | ms/batch 15.04 | loss  5.93 | ppl   375.81\n",
            "| epoch   1 |  2600/ 2928 batches | lr 4.29 | ms/batch 14.95 | loss  5.92 | ppl   372.13\n",
            "| epoch   1 |  2800/ 2928 batches | lr 4.29 | ms/batch 15.31 | loss  5.84 | ppl   344.34\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   2 |   200/ 2928 batches | lr 4.07 | ms/batch 14.97 | loss  5.84 | ppl   345.15\n",
            "| epoch   2 |   400/ 2928 batches | lr 4.07 | ms/batch 14.82 | loss  5.80 | ppl   331.88\n",
            "| epoch   2 |   600/ 2928 batches | lr 4.07 | ms/batch 14.84 | loss  5.58 | ppl   266.13\n",
            "| epoch   2 |   800/ 2928 batches | lr 4.07 | ms/batch 15.27 | loss  5.60 | ppl   270.89\n",
            "| epoch   2 |  1000/ 2928 batches | lr 4.07 | ms/batch 15.03 | loss  5.56 | ppl   258.95\n",
            "| epoch   2 |  1200/ 2928 batches | lr 4.07 | ms/batch 14.76 | loss  5.60 | ppl   269.39\n",
            "| epoch   2 |  1400/ 2928 batches | lr 4.07 | ms/batch 14.79 | loss  5.62 | ppl   275.48\n",
            "| epoch   2 |  1600/ 2928 batches | lr 4.07 | ms/batch 14.92 | loss  5.65 | ppl   284.40\n",
            "| epoch   2 |  1800/ 2928 batches | lr 4.07 | ms/batch 15.28 | loss  5.59 | ppl   266.66\n",
            "| epoch   2 |  2000/ 2928 batches | lr 4.07 | ms/batch 14.78 | loss  5.59 | ppl   266.60\n",
            "| epoch   2 |  2200/ 2928 batches | lr 4.07 | ms/batch 14.77 | loss  5.49 | ppl   242.56\n",
            "| epoch   2 |  2400/ 2928 batches | lr 4.07 | ms/batch 14.81 | loss  5.58 | ppl   263.81\n",
            "| epoch   2 |  2600/ 2928 batches | lr 4.07 | ms/batch 15.18 | loss  5.59 | ppl   268.86\n",
            "| epoch   2 |  2800/ 2928 batches | lr 4.07 | ms/batch 15.14 | loss  5.53 | ppl   251.76\n",
            "| epoch   3 |   200/ 2928 batches | lr 3.87 | ms/batch 14.83 | loss  5.56 | ppl   260.86\n",
            "| epoch   3 |   400/ 2928 batches | lr 3.87 | ms/batch 14.86 | loss  5.56 | ppl   260.08\n",
            "| epoch   3 |   600/ 2928 batches | lr 3.87 | ms/batch 15.31 | loss  5.34 | ppl   208.53\n",
            "| epoch   3 |   800/ 2928 batches | lr 3.87 | ms/batch 15.21 | loss  5.37 | ppl   215.72\n",
            "| epoch   3 |  1000/ 2928 batches | lr 3.87 | ms/batch 14.89 | loss  5.34 | ppl   208.65\n",
            "| epoch   3 |  1200/ 2928 batches | lr 3.87 | ms/batch 14.84 | loss  5.39 | ppl   218.61\n",
            "| epoch   3 |  1400/ 2928 batches | lr 3.87 | ms/batch 14.97 | loss  5.40 | ppl   222.11\n",
            "| epoch   3 |  1600/ 2928 batches | lr 3.87 | ms/batch 15.41 | loss  5.45 | ppl   232.07\n",
            "| epoch   3 |  1800/ 2928 batches | lr 3.87 | ms/batch 14.89 | loss  5.38 | ppl   218.01\n",
            "| epoch   3 |  2000/ 2928 batches | lr 3.87 | ms/batch 14.89 | loss  5.39 | ppl   219.94\n",
            "| epoch   3 |  2200/ 2928 batches | lr 3.87 | ms/batch 14.89 | loss  5.29 | ppl   198.36\n",
            "| epoch   3 |  2400/ 2928 batches | lr 3.87 | ms/batch 15.26 | loss  5.39 | ppl   219.97\n",
            "| epoch   3 |  2600/ 2928 batches | lr 3.87 | ms/batch 15.25 | loss  5.41 | ppl   223.24\n",
            "| epoch   3 |  2800/ 2928 batches | lr 3.87 | ms/batch 14.90 | loss  5.35 | ppl   210.41\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        train(tutorial_model)\n",
        "        scheduler2.step()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3Vm_ljhwjE-m"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.6 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "8f1af38ef70d2ba974426d7a8a6b596b940002c76a66dd7cd7f620996d04d624"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}